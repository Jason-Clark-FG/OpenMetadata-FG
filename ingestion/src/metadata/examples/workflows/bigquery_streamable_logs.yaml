# Example workflow configuration with streamable logging enabled
# 
# To enable streamable logging for ingestion pipelines:
# 1. Set the environment variable: ENABLE_STREAMABLE_LOGS=true
# 2. Ensure the server has log storage configured (S3 or default)
# 3. Run the workflow with ingestionPipelineFQN and pipelineRunId
#
# The logs will be automatically streamed to the server and stored
# based on the configured log storage backend.

source:
  type: bigquery
  serviceName: local_bigquery
  serviceConnection:
    config:
      type: BigQuery
      taxonomyProjectID: [ project-id-where-policy-tags-exist ]
      credentials:
        gcpConfig:
          type: service_account
          projectId: project_id
          privateKeyId: private_key_id
          privateKey: private_key
          clientEmail: gcpuser@project_id.iam.gserviceaccount.com
          clientId: client_id
          authUri: https://accounts.google.com/o/oauth2/auth
          tokenUri: https://oauth2.googleapis.com/token
          authProviderX509CertUrl: https://www.googleapis.com/oauth2/v1/certs
          clientX509CertUrl: https://www.googleapis.com/oauth2/v1/certs

  sourceConfig:
    config:
      type: DatabaseMetadata

sink:
  type: metadata-rest
  config: {}

workflowConfig:
  # Set the logger level - this applies to both console and streamable logging
  loggerLevel: INFO # DEBUG, INFO, WARN or ERROR
  
  openMetadataServerConfig:
    hostPort: http://localhost:8585/api
    authProvider: openmetadata
    securityConfig:
      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"

# When running with Airflow or another orchestrator, these will be automatically set:
# ingestionPipelineFQN: service.pipeline_name
# pipelineRunId: <auto-generated-uuid>

# To test streamable logging locally:
# 1. Set environment variable:
#    export ENABLE_STREAMABLE_LOGS=true
# 
# 2. Run the workflow:
#    metadata ingest -c bigquery_streamable_logs.yaml
#
# The logs will be streamed to the server if:
# - ENABLE_STREAMABLE_LOGS is set to true
# - The server has log storage configured
# - ingestionPipelineFQN and pipelineRunId are provided (automatic in production)